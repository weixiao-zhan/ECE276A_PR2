\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{extarrows}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{ECE276A PR2 Report}

\author{
\IEEEauthorblockN{1\textsuperscript{st} Weixiao Zhan}
\IEEEauthorblockA{
    weixiao-zhan[at]ucsd[dot]edu}
}

\maketitle


\section{Introduction}
Simultaneous Localization and Mapping (SLAM) is pivotal in robotics, 
enabling autonomous systems to understand their surroundings and their locations 
with minimal human intervention. 
In this project, I implemented SLAM on a differential-drive robot equipped with a wheel encoder, 
an Inertial Measurement Unit (IMU), 2-D LiDAR, and an RGBD camera, 
and reconstructed detailed 2D occupancy map, 2D floor texture map, and 3D texture map.

The process began with the construction of a motion model trajectory based on 
the differential-drive kinematics, 
utilizing data from the wheel encoder and IMU. 
Concurrently, an observation model trajectory was estimated using LiDAR scans 
and the Iterative Closest Points (ICP) method. 
Subsequently, I applied a Factor Graph and loop closure techniques to 
refine and optimize the trajectory. 
Lastly, I leveraged the optimized trajectory to generate 
detailed maps, showcasing the potential.


\section{Problem Formulation}

In this project, the robot trajectory, discretized as a set of pose and time stamp tuples, is the core of all computation.
Denote the robot's pose at given time stamp \(t\) as \(P_t\).
The pose can be represented in both two-dimensional (2D) and three-dimensional (3D) spaces, 
with the z-axis value fixed at zero. 
$$
P_{t} = \begin{cases}
    \xlongequal{2D} \left[ \begin{gathered}x\\ y\\ \theta \end{gathered} \right] & \in \mathbb{R}^{3} \\ 
    \xlongequal{3D} \left[ \begin{matrix}R_{yaw}(\theta )&\left[ \begin{gathered}x\\ y\\0\end{gathered} \right]  \\ \mathbf{0}&1\end{matrix} \right] & \in \mathbb{R}^{[4\times 4]}
\end{cases} 
$$
This dual representation enables seamless transition between 2D and 3D spatial analyses.
The initial pose $P_0$ at time 0 is defined as the origin of the world frame
serving as the reference for all subsequent poses.
$$P_{0}=\begin{cases}\vec{0}_{3}\\ I_{4}\end{cases} $$


\subsection{Motion Model: IMU \& Differential-drive Kinematics}
Consider a small time interval $\tau$ between two time stamp $t$ and $t+1$.
Suppose wheel encoder reported $\delta d$
and IMU's yaw reading reproted $\delta \theta$ during this interval,
the motion of robot can be approximate as a arc:
$$
\begin{aligned}
    P_{t+1}^{(2M)}
    % &=P_{t}^{(2M)} +\tau \left[ \begin{gathered}v_{t}\mathrm{sinc} \left( \frac{w_{t}\tau }{2} \right)  \cos \left( \theta_{t} +\frac{w_{t}\tau }{2} \right)  \\ v_{t}\mathrm{sinc} \left( \frac{w_{t}\tau }{2} \right)  \sin \left( \theta_{t} +\frac{w_{t}\tau }{2} \right)  \\ w_{t}\end{gathered} \right]  \\
    &=P_{t}^{(2M)} +\left[ \begin{gathered}\delta d\cdot \mathrm{sinc} \left( \frac{\delta \theta }{2} \right)  \sin \left( \theta_{t} +\frac{\delta \theta }{2} \right)  \\ \delta d\cdot \mathrm{sinc} \left( \frac{\delta \theta }{2} \right)  \sin \left( \theta_{t} +\frac{\delta \theta }{2} \right)  \\ \delta \theta \end{gathered} \right]
\end{aligned} 
$$
in which, the superscript $^{(2M)}$ means this is a 2D motion model trajectory,


\subsection{Observation Model: LiDAR \& ICP}
The ICP algorithm can estimate relative transformation between 
two given 3D point cloud: source $S$ and target $T$.
It achieved this by iterates over these steps:
\begin{enumerate}
    \item Find point association for all source points using closest distance:
    $$ \Delta = \left\{\left(s, \arg\min_{t\in T} \|s - t\| \right) | \forall s\in S\right\}$$

    \item Estimate relative transformation $_tT_s$ using Kabsch algorithm:
    $$_tT_s = \arg\min_T \sum\limits_{(s,t)\in\Delta} \|t - Ts\|$$

    \item Update $S \leftarrow {}_tT_s S$ and repeat step 1 until no improvement.
\end{enumerate}

Meanwhile, 2D-LiDAR scan reports a set of range and angle tuple $L = \{(r_i, \phi_i)\}$,
which can be converted to a set of 3D point cloud in body frame using following equations.
$$
\begin{aligned}
    \mathrm{PC}_{\text{LiDAR frame}} 
        &= \left\{ \left[ \begin{gathered}r_i\sin \phi_i \\ r_i\cos \phi_i \\ 0\end{gathered} \right] | \forall (r_i, \phi_i)\in L\right\} 
        \\
    \left[ \begin{gathered} \mathrm{PC} \\ 1\end{gathered} \right]  
    &= _{\text{body}}T_{\text{LidAR}}  \left[ \begin{gathered} \mathrm{PC}_{\text{LidAR frame}}  \\ 1\end{gathered} \right]  
\end{aligned}
$$

Leveraging ICP, I can estimate relative transformation between two LiDAR scan and
compose observation model trajectory:
$$
\begin{aligned}
    _{t}T^{(O)}_{t+1}
        &=\text{ICP}(S=\mathrm{PC}_{t+1}, T=\mathrm{PC}_{t}) \\ 
    P^{(3O)}_{t}
        &=P_{0}\prod^{t}_{f=1} {}_{f-1} T^{(O)}_{f}
\end{aligned} 
$$
in which, the superscript $^{(3O)}$ means this is a 3D observation trajectory.

\subsection{Factor Graph and Loop Closure}

\subsection{Mapping}

\subsubsection{Occupancy}


\subsubsection{Texture}


\section{Technical Approach}
[25 pts] Technical Approach: Describe your technical approach to SLAM and texture mapping.
\subsection{Time Stamp}
interpolation 
linear 
cubic
\subsection{wheel encoder}
average of all 4 wheels
\subsection{2D 3D pose conversion}


\subsection{ICP}
partial ICP

\subsection{RGBD}
distance hist and remove


\section{Results}
[15 pts] Results: Present your results, and discuss them â€“ what worked, what did not, and why.
Analyze the impact of loop closure detection and pose graph optimization on the accuracy of the robot
trajectory estimate and the resulting map quality. Make sure your results include (a) images of the
trajectory and occupancy grid map over time constructed by your SLAM algorithm and (b) textured
maps over time. If you have videos, include them in your submission zip file and refer to them in your
report.


\end{document}
